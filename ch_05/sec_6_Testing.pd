\section {Тестирование} \label{ch05-testing}

Как было изложено в разделе \ref{ch04-testing}, для тестирования нами было выбрано случайное подмножество из 100 синсетов PWN, для 97 из которых было найдено хотя бы по одну кандидату на связывание. В этом разделе мы опишем ход и результаты второго этапа тестирования, исследующего эффективность процедуры выравнивания в целом (т.е. комбинацию автоматического выравнивания и применения краудсорсинга).

В процессе тестирования было проведено два раунда. В первом было обработано 159 заданий, во втором --- 57 заданий. В обоих раундах было использовано перекрытие (количество раз, которое будет выполнено каждое задание разными рабочими) 5.

Задания для первого раунда были сформированы на основании графа связей. 16 из 57 заданий второго раунда также были получены только на основании связей, полученных автоматическим выравниванием (синсеты этих заданий не были включены в первый раунд из организационных соображений), другие 39 заданий второго раунда были сгенерированны на основании результатов первого раунда.

Как было отмечено в \ref{ch04-testing}, тестирование метода проводилось путем сравнения с эталонной разметкой. С помощью краудсорсинга мы получили для каждого исходного синсета из PWN от 0 до 2 синсетов-победителей, сравнили полученные данные c эталоном. В \inlref{таблице}{tbl:testing-result} представлено сравнение результатов.

\begin{table}[tb]
\caption{Сравнение результатов краудсорсинга и экспертной разметки.\label{tbl:testing-result}}
\begin{tabular}{|l r|}
\toprule
 Результат & Кол-во \\
\midrule
 Нет перевода & 3 \\
 Неверная классификация, "ни один не подходит" & 1 \\
 Неверная классификация, нерелевантный синсет & 12 \\
 Согласующаяся классификация, "ни один не подходит" & 5 \\
 Согласующаяся классификация, релевантный синсет & 79 \\
\bottomrule
\end{tabular}
\end{table}

Причем, из 79 классифицированных согласующимся с мнением эксперта образом синсетов, 56 получившихся классификаций были отмечены экспертом, как полностью соответствующие.
Остальные 23 установленные связи требуют дополнительного уточнения синсетов из YARN, чтобы соответствующие синсеты также полностью соответствовали понятиям, описанным исходными синсетами PWN.

Однако установленные связи с синсетами, не полностью соответствующими понятиям (но достаточно близкими) --- еще не столь большая проблема. То же касается и синсетов, для которых неверно помечено, что ни один кандидат не подходит. Гораздо более значительную проблему представляют синсеты, для которых были выбраны некорректные кандидаты. Мы подробно проанализировали причины неверной классификации:

* смешение понятий в синсете YARN: 1
* выбран нерелевантный синсет при отсутствии релевантного в выборке
    + выбран синсет, соответствующий английскому понятию, имеющим общие слова с исходным: 2
* выбран нерелевантный синсет при наличии релевантного в выборке
    + выбран синсет, соответствующий английскому понятию, имеющим общие слова с исходным: 5
    + выбран слишком общий гипероним: 1
    + выбран слишком частный гипоним: 3
* не выбрано ни одного при наличии релевантного синсета в выборке: 1

Что замечено, часто синсеты успешно проходили первый шаг (т.е. в множестве кандидатов по результатам голосования на первом шаге оставались в том числе релевантные синсеты), однако на втором участники голосования выбирали ошибочный синсет. На самом деле 12 синсетов (или 12% тестовой выборки) --- достаточно неплохой результат. Однако для построения тезауруса (а задача выравнивания формулируется именно в как подзадача задачи построения тезауруса YARN) точность 88% не является
удовлетворительной.

### Сравнение с результатами других работ ###

Чтобы сравнить полученный результат с результатами, полученными авторами других работ, посчитаем значения точности (precision) и чувствительности (recall) по следующим формулам ($\operatorname{found}$ --- найденные сопоставления, $\operatorname{correct}$ --- корректные сопоставления):

\begin{equation*}
  \operatorname{recall} = \frac{|\operatorname{found} \cap \operatorname{correct}|}{|\operatorname{found}|}
\end{equation*}

\begin{equation*}
  \operatorname{precision} = \frac{|\operatorname{found} \cap \operatorname{correct}|}{|\operatorname{correct}|}
\end{equation*}

Получаем значения $\operatorname{precision} = 0.91$, $\operatorname{recall} = 0.86$.

Известные нам попытки решения задачи выравнивания тезаурусов различных языков были рассмотрены в разделе \ref{ch02-prev}, все они производились силами экспертов.
Существует множество работ, посвященных автоматическому выравниванию тезаурусов, онтологий созданных на одном языке. Проводились исследования и по автоматическому переводу тезаурусов в другие языки (именно перевода тезауруса в другой язык, а не сопоставления двух независимо созданных тезаурусов).

Авторы работы [@agrovoc-alignment] сравнивают эффективность различных методов в задаче соспоставления записей тезаурусов агрономической области. Заметим, что авторами рассматривается задача сопоставления тезаурусов для английского языка в узкой области применения (что значительно отличается от задачи, решаемой нами). Наилучшие значения точности, чувствительности, полученные в ходе их эксперимента: $\operatorname{precision} = 0.84$,
$\operatorname{recall} = 0.49$.

Авторы работы [@wn-to-french] поставили своей задачей перевод тезауруса PWN на французский язык. Ими был применен подход с использованием машинного обучения, в котором определялись корректные пары $(n, t)$, где $n$ --- синсет PWN, $t$ --- слово французского языка. Авторы измерили значения чувствительности, точности для
разных частей речи и в частности для существительных ими были получены значения $\operatorname{precision} = 0.84$, $\operatorname{recall} = 0.88$.

Таким образом, можно судить о достаточной эффективности полученного метода в сравнении с результатами аналогичных работ.
В следующем разделе мы рассмотрим подходы, применяя которые мы надеемся улучшить полученный результат (однако применение которых выходит за рамки настоящей работы).
